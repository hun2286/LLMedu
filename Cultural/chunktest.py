import os
import fitz
from dotenv import load_dotenv
from langchain_community.vectorstores import Chroma
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_openai import ChatOpenAI
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain.schema import Document, SystemMessage, HumanMessage

# ========================
# í™˜ê²½ ì„¤ì •
# ========================
load_dotenv()
api_key = os.getenv("OPENAI_API_KEY")

pdf_folder = r"C:\Users\BGR_NC_2_NOTE\Desktop\pdfs\20251106" 
persist_base_dir = r"./vector_dbs1"
os.makedirs(persist_base_dir, exist_ok=True)

# LLM / ìž„ë² ë”© ì„¤ì •
llm = ChatOpenAI(
    model="gpt-4o-mini",
    temperature=0.2,
    max_tokens=8000,
    openai_api_key=api_key
)

embedding_model = HuggingFaceEmbeddings(
    model_name="bespin-global/klue-sroberta-base-continue-learning-by-mnr"
)

# ========================
# PDF â†’ Document ë³€í™˜
# ========================
def pdf_to_markdown(pdf_path):
    md_text = ""
    try:
        with fitz.open(pdf_path) as pdf:
            for i, page in enumerate(pdf):
                blocks = page.get_text("blocks")
                blocks = sorted(blocks, key=lambda b: (b[1], b[0]))
                md_text += f"# Page {i + 1}\n\n"
                for block in blocks:
                    text = block[4].strip()
                    if not text:
                        continue
                    num_words = len(text.split())
                    num_lines = text.count("\n") + 1
                    if num_words <= 5 and num_lines <= 2:
                        md_text += f"# {text}\n\n"
                    elif num_words <= 15:
                        md_text += f"## {text}\n\n"
                    else:
                        md_text += f"{text}\n\n"
        return md_text
    except Exception:
        print(f"[ì˜¤ë¥˜] PDF ë³€í™˜ ì‹¤íŒ¨ : {pdf_path}\n")
        return ""

def load_pdf_safe(pdf_path):
    md_text = pdf_to_markdown(pdf_path)
    if md_text:
        return [Document(page_content=md_text,
                         metadata={"source": os.path.splitext(os.path.basename(pdf_path))[0]})]
    return []

def load_all_pdfs(pdf_folder):
    all_docs = []
    pdf_files = [f for f in os.listdir(pdf_folder) if f.lower().endswith(".pdf")]
    print(f"ì´ PDF íŒŒì¼ ìˆ˜: {len(pdf_files)}\n")
    for pdf_file in pdf_files:
        pdf_path = os.path.join(pdf_folder, pdf_file)
        try:
            docs = load_pdf_safe(pdf_path)
            all_docs.extend(docs)
            print(f"{pdf_file} ì²˜ë¦¬ ì™„ë£Œ ({len(docs)} ë¬¸ì„œ)")
        except Exception as e:
            print(f"{pdf_file} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
    print(f"\nì´ Document ìˆ˜: {len(all_docs)}")
    return all_docs

# ========================
# ì²­í¬ ì„¤ì • (2ê°œë§Œ)
# ========================
chunk_settings = [
    {"size": 1000, "overlap": 200},
    {"size": 1200, "overlap": 300},
]

docs = None
vectorstores = []

# ========================
# DB ìƒì„± + ì²­í¬ ë‚´ìš© ì „ì²´ ì €ìž¥
# ========================
for setting in chunk_settings:
    db_name = f"test_{setting['size']}_{setting['overlap']}"
    db_dir = os.path.join(persist_base_dir, db_name)
    os.makedirs(db_dir, exist_ok=True)

    print(f"\n=== [{db_name}] DB ìƒì„± ì‹œìž‘ ===")

    # PDF ë¡œë“œ (ìµœì´ˆ 1íšŒ)
    if docs is None:
        docs = load_all_pdfs(pdf_folder)

    # ì²­í¬ ë¶„í• 
    splitter = RecursiveCharacterTextSplitter(
        chunk_size=setting["size"],
        chunk_overlap=setting["overlap"]
    )
    split_docs = splitter.split_documents(docs)
    print(f"ì´ {len(split_docs)}ê°œì˜ ì²­í¬ ìƒì„± ì™„ë£Œ.")

    # ë²¡í„° DB ìƒì„±
    vectorstore = Chroma.from_documents(
        documents=split_docs,
        embedding=embedding_model,
        persist_directory=db_dir
    )
    vectorstore.persist()
    print(f"âœ… ë²¡í„° DB ì €ìž¥ ì™„ë£Œ: {db_dir}")

    # ì²­í¬ ì „ì²´ ë‚´ìš© ì €ìž¥
    preview_path = os.path.join(db_dir, f"{db_name}_chunks_full.txt")
    with open(preview_path, "w", encoding="utf-8") as f:
        for i, chunk in enumerate(split_docs):
            f.write(f"--- ì²­í¬ {i + 1} ---\n")
            f.write(chunk.page_content.strip() + "\n")
            f.write(f"[ì¶œì²˜: {chunk.metadata.get('source', 'ì—†ìŒ')}]\n")
            f.write("=" * 80 + "\n")
    print(f"ðŸ“„ ì²­í¬ ì „ì²´ ë‚´ìš© ì €ìž¥ ì™„ë£Œ: {preview_path}")

    vectorstores.append((vectorstore, setting, len(split_docs)))

print("\nâœ… ëª¨ë“  DB ë° ì²­í¬ ë‚´ìš© ì €ìž¥ ì™„ë£Œ!")

# ========================
# RAG ì§ˆì˜ì‘ë‹µ
# ========================
def rag_answer(question, retriever):
    retriever_docs = retriever.invoke(question)
    if isinstance(retriever_docs, Document):
        retriever_docs = [retriever_docs]

    context_texts = [doc.page_content.strip() for doc in retriever_docs if doc.page_content.strip()]
    context = "\n\n".join(context_texts)
    sources = sorted(set([doc.metadata.get("source", "ì¶œì²˜ ì—†ìŒ") for doc in retriever_docs]))

    messages = [
        SystemMessage(content="""
            ë‹¹ì‹ ì€ ì—¬ëŸ¬ PDF ë¬¸ì„œë¥¼ ì°¸ê³ í•˜ì—¬ ì§ˆë¬¸ì— ë‹µí•˜ëŠ” ì „ë¬¸ê°€ìž…ë‹ˆë‹¤.
            - ë¬¸ì„œì— ì—†ëŠ” ë‚´ìš©ì€ ì ˆëŒ€ ì¶”ê°€í•˜ì§€ ë§ê³ , ì—†ìœ¼ë©´ 'ì •ë³´ ì—†ìŒ'ì´ë¼ê³  í‘œì‹œí•˜ì„¸ìš”.
            - ë‹µë³€ì€ í•­ëª©ë³„ë¡œ êµ¬ë¶„ëœ í˜•íƒœë¡œ ìž‘ì„±í•˜ì„¸ìš”.
            - ê° í•­ëª©ì€ í•œ ì¤„ ë„ìš°ê¸°ë¡œ êµ¬ë¶„
        """),
        HumanMessage(content=f"ë¬¸ì„œ ë‚´ìš©:\n{context}\n\nì§ˆë¬¸:\n{question}")
    ]

    response = llm.invoke(messages)
    answer = response.content

    # í˜•ì‹ ì •ë¦¬
    lines = answer.split("\n")
    final_lines, counter = [], 1
    for line in lines:
        stripped = line.strip()
        if not stripped:
            final_lines.append("")
            continue
        if stripped.startswith("#"):
            stripped = stripped.lstrip("#").strip()
            final_lines.append(f"{counter}. {stripped}")
            counter += 1
        else:
            final_lines.append(stripped)

    if sources:
        final_lines.append("\n---ì¶œì²˜---")
        for s in sources:
            final_lines.append(f"[ì¶œì²˜: {s}]")

    return "\n".join(final_lines), len(retriever_docs)

# ========================
# ì§ˆì˜ì‘ë‹µ ì‹¤í–‰
# ========================
if __name__ == "__main__":
    while True:
        question = input("\nì§ˆë¬¸ì„ ìž…ë ¥í•˜ì„¸ìš” (exit ìž…ë ¥ ì‹œ ì¢…ë£Œ): ").strip()
        if question.lower() == "exit":
            print("í”„ë¡œê·¸ëž¨ ì¢…ë£Œ.")
            break

        for vectorstore, setting, total_chunks in vectorstores:
            retriever = vectorstore.as_retriever(search_type="similarity", search_kwargs={"k": 5})
            answer, retrieved_count = rag_answer(question, retriever)

            print(f"\n=== [{setting['size']}_{setting['overlap']}] ê²°ê³¼ ===")
            print(f"ì´ ì²­í¬ ìˆ˜: {total_chunks}")
            print(f"ê²€ìƒ‰ëœ ì²­í¬ ìˆ˜: {retrieved_count}")
            print(f"ë‹µë³€ ê¸¸ì´: {len(answer)} ê¸€ìž\n")
            print(answer)
            print("-" * 60)
