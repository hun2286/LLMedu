import os
import fitz
import re
from dotenv import load_dotenv
from PIL import Image
import pytesseract

from langchain_community.vectorstores import Chroma
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_openai import ChatOpenAI
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain.schema import Document, SystemMessage, HumanMessage

# ========================
# í™˜ê²½ ì„¤ì •
# ========================
load_dotenv()
api_key = os.getenv("OPENAI_API_KEY")

persist_base_dir = r"./vector_dbs1"
os.makedirs(persist_base_dir, exist_ok=True)

# tesseract.exe ê²½ë¡œ
pytesseract.pytesseract.tesseract_cmd = r"C:\Program Files\Tesseract-OCR\tesseract.exe"
# tessdata ê²½ë¡œ (kor.traineddataì™€ í•œì í¬í•¨ kor_vert ë“± í•„ìš”)
os.environ["TESSDATA_PREFIX"] = r"C:\Program Files\Tesseract-OCR\tessdata"

# LLM / ì„ë² ë”© ì„¤ì •
llm = ChatOpenAI(
    model="gpt-4o-mini",
    temperature=0.2,
    max_tokens=8000,
    openai_api_key=api_key
)

embedding_model = HuggingFaceEmbeddings(
    model_name="bespin-global/klue-sroberta-base-continue-learning-by-mnr"
)

# ========================
# PDF â†’ í…ìŠ¤íŠ¸ / OCR ë³€í™˜ (ê°œì„ )
# ========================
def pdf_to_text_or_ocr(pdf_path):
    text = ""
    try:
        with fitz.open(pdf_path) as pdf:
            for page in pdf:
                page_text = page.get_text()
                text += page_text + "\n"
    except Exception as e:
        print(f"[PyMuPDF ì˜¤ë¥˜] {pdf_path}: {e}")

    # í…ìŠ¤íŠ¸ ì—†ê±°ë‚˜ ë„ˆë¬´ ì§§ìœ¼ë©´ OCR ìˆ˜í–‰
    if not text.strip() or len(text.strip()) < 20:
        print("[OCR ì‹¤í–‰] í…ìŠ¤íŠ¸ ì—†ìŒ ë˜ëŠ” ë„ˆë¬´ ì§§ìŒ, ì´ë¯¸ì§€ì—ì„œ ì¶”ì¶œ ì¤‘:", pdf_path)
        try:
            with fitz.open(pdf_path) as pdf:
                for page in pdf:
                    # ê³ í•´ìƒë„ ì´ë¯¸ì§€ ë³€í™˜
                    pix = page.get_pixmap(dpi=300)
                    img = Image.frombytes("RGB", [pix.width, pix.height], pix.samples)

                    # í‘ë°± ë³€í™˜
                    img = img.convert("L")

                    # Tesseract ì˜µì…˜: í•œê¸€ + ë²ˆì²´ í•œì
                    custom_config = r'--oem 3 --psm 6'
                    page_text = pytesseract.image_to_string(img, lang="kor+chi_tra", config=custom_config)

                    # ì¤„ë°”ê¿ˆ, ê³µë°± ì •ë¦¬
                    lines = page_text.splitlines()
                    cleaned_lines = []

                    for line in lines:
                        line = line.strip()
                        if not line:
                            continue

                        # ê´„í˜¸ ë‚´ë¶€ í•œìë§Œ í•„í„°ë§
                        def preserve_parentheses(match):
                            content = match.group(1)
                            # í•œì, ìˆ«ì, ì˜ì–´ë§Œ í—ˆìš© (í•œê¸€ ì œì™¸)
                            filtered = re.sub(r"[^ä¸€-é¾¥0-9a-zA-Z]", "", content)
                            return f"({filtered})"

                        line = re.sub(r"\((.*?)\)", preserve_parentheses, line)

                        # ê´„í˜¸ ë°– ê¹¨ì§„ ë¬¸ì ì œê±° (í•œê¸€, ì˜ì–´, ìˆ«ì, ê¸°ë³¸ êµ¬ë‘ì )
                        line = re.sub(r"[^ê°€-í£0-9a-zA-Z\s.,:;!?()\[\]\-<>%]", "", line)

                        if len(line) >= 2:
                            cleaned_lines.append(line)

                    page_text = "\n".join(cleaned_lines)
                    text += page_text + "\n"

        except Exception as e:
            print(f"[OCR ì˜¤ë¥˜] {pdf_path}: {e}")

    return text.strip()
def load_pdf_safe(pdf_path):
    content = pdf_to_text_or_ocr(pdf_path)
    if content:
        return [Document(page_content=content,
                         metadata={"source": os.path.splitext(os.path.basename(pdf_path))[0]})]
    return []

# ========================
# ë‹¨ì¼ PDF / í´ë” PDF ì²˜ë¦¬
# ========================
def process_pdf_folder(pdf_folder, chunk_size=1200, chunk_overlap=300):
    all_docs = []
    pdf_files = [f for f in os.listdir(pdf_folder) if f.lower().endswith(".pdf")]
    print(f"ì´ PDF íŒŒì¼ ìˆ˜: {len(pdf_files)}\n")
    for pdf_file in pdf_files:
        pdf_path = os.path.join(pdf_folder, pdf_file)
        docs = load_pdf_safe(pdf_path)
        all_docs.extend(docs)
        print(f"{pdf_file} ì²˜ë¦¬ ì™„ë£Œ ({len(docs)} ë¬¸ì„œ)")
    return create_vectorstore(all_docs, chunk_size, chunk_overlap, name_prefix="folder")

def process_single_pdf(pdf_path, chunk_size=1200, chunk_overlap=300):
    docs = load_pdf_safe(pdf_path)
    if not docs:
        print("[ê²½ê³ ] PDFì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŒ:", pdf_path)
        return None
    return create_vectorstore(docs, chunk_size, chunk_overlap, name_prefix="single")

# ========================
# ë²¡í„° DB ìƒì„± + ì²­í¬ ì €ì¥
# ========================
def create_vectorstore(docs, chunk_size, chunk_overlap, name_prefix):
    splitter = RecursiveCharacterTextSplitter(
        chunk_size=chunk_size,
        chunk_overlap=chunk_overlap
    )
    split_docs = splitter.split_documents(docs)
    print(f"ì´ {len(split_docs)}ê°œì˜ ì²­í¬ ìƒì„± ì™„ë£Œ.")

    db_name = f"{name_prefix}_vector_db"
    db_dir = os.path.join(persist_base_dir, db_name)
    os.makedirs(db_dir, exist_ok=True)

    vectorstore = Chroma.from_documents(
        documents=split_docs,
        embedding=embedding_model,
        persist_directory=db_dir
    )
    vectorstore.persist()
    print(f"âœ… ë²¡í„° DB ì €ì¥ ì™„ë£Œ: {db_dir}")

    # ì²­í¬ ì „ì²´ ì €ì¥
    preview_path = os.path.join(db_dir, f"{db_name}_chunks_full.txt")
    with open(preview_path, "w", encoding="utf-8") as f:
        for i, chunk in enumerate(split_docs):
            f.write(f"--- ì²­í¬ {i + 1} ---\n")
            f.write(chunk.page_content.strip() + "\n")
            f.write(f"[ì¶œì²˜: {chunk.metadata.get('source', 'ì—†ìŒ')}]\n")
            f.write("="*80 + "\n")
    print(f"ğŸ“„ ì²­í¬ ì „ì²´ ë‚´ìš© ì €ì¥ ì™„ë£Œ: {preview_path}")
    return vectorstore

# ========================
# RAG ì§ˆì˜ì‘ë‹µ
# ========================
def rag_answer(question, retriever):
    retriever_docs = retriever.invoke(question)
    if isinstance(retriever_docs, Document):
        retriever_docs = [retriever_docs]

    context_texts = [doc.page_content.strip() for doc in retriever_docs if doc.page_content.strip()]
    context = "\n\n".join(context_texts)
    sources = sorted(set([doc.metadata.get("source", "ì¶œì²˜ ì—†ìŒ") for doc in retriever_docs]))

    messages = [
        SystemMessage(content="""
            ë‹¹ì‹ ì€ ì—¬ëŸ¬ PDF ë¬¸ì„œë¥¼ ì°¸ê³ í•˜ì—¬ ì§ˆë¬¸ì— ë‹µí•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
            - ë¬¸ì„œì— ì—†ëŠ” ë‚´ìš©ì€ ì ˆëŒ€ ì¶”ê°€í•˜ì§€ ë§ê³ , ì—†ìœ¼ë©´ 'ì •ë³´ ì—†ìŒ'ì´ë¼ê³  í‘œì‹œí•˜ì„¸ìš”.
            - ë‹µë³€ì€ í•­ëª©ë³„ë¡œ êµ¬ë¶„ëœ í˜•íƒœë¡œ ì‘ì„±í•˜ì„¸ìš”.
            - ê° í•­ëª©ì€ í•œ ì¤„ ë„ìš°ê¸°ë¡œ êµ¬ë¶„
        """),
        HumanMessage(content=f"ë¬¸ì„œ ë‚´ìš©:\n{context}\n\nì§ˆë¬¸:\n{question}")
    ]

    response = llm.invoke(messages)
    answer = response.content

    # í˜•ì‹ ì •ë¦¬
    lines = answer.split("\n")
    final_lines, counter = [], 1
    for line in lines:
        stripped = line.strip()
        if not stripped:
            final_lines.append("")
            continue
        if stripped.startswith("#"):
            stripped = stripped.lstrip("#").strip()
            final_lines.append(f"{counter}. {stripped}")
            counter += 1
        else:
            final_lines.append(stripped)

    if sources:
        final_lines.append("\n---ì¶œì²˜---")
        for s in sources:
            final_lines.append(f"[ì¶œì²˜: {s}]")

    return "\n".join(final_lines), len(retriever_docs)

# ========================
# ì‹¤í–‰ ì„ íƒ
# ========================
if __name__ == "__main__":
    print("1. PDF í´ë” ì „ì²´ ì²˜ë¦¬\n2. ë‹¨ì¼ PDF ì²˜ë¦¬")
    choice = input("ì„ íƒí•˜ì„¸ìš” (1 ë˜ëŠ” 2): ").strip()

    if choice == "1":
        pdf_folder = r"C:\Users\BGR_NC_2_NOTE\Desktop\pdfs\20251106"
        vectorstore = process_pdf_folder(pdf_folder)
    elif choice == "2":
        pdf_path = r"C:\Users\BGR_NC_2_NOTE\Desktop\pdfs\20251106\1999_ê²½ê¸°ë„ë„ë‹¹êµ¿_05_â…¡_ê²½ê¸°ë„ ë„ë‹¹êµ¿ì˜ ë‚´ìš©_19.pdf"
        vectorstore = process_single_pdf(pdf_path)
    else:
        print("ì˜ëª»ëœ ì„ íƒì…ë‹ˆë‹¤.")
        exit()

    if vectorstore:
        retriever = vectorstore.as_retriever(search_type="similarity", search_kwargs={"k": 5})
        while True:
            question = input("\nì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš” (exit ì…ë ¥ ì‹œ ì¢…ë£Œ): ").strip()
            if question.lower() == "exit":
                break
            answer, retrieved_count = rag_answer(question, retriever)
            print(f"\nê²€ìƒ‰ëœ ì²­í¬ ìˆ˜: {retrieved_count}")
            print(answer)
            print("-" * 60)
