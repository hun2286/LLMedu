# ìŠ¤ìº”ë³¸ ì œì™¸í•˜ê³  276ê°œ pdf ragì‘ì—… í•´ë³´ëŠ” ì½”ë“œ
# ìŠ¤ìº”ë³¸ì€ í…ìŠ¤íŠ¸ ê¸¸ì´ íŒë‹¨í•´ì„œ ì‘ìœ¼ë©´ ë°˜í™˜ê°’ì„ ëª©ë¡ì— ë„£ì–´ì„œ ì•Œë ¤ì¤Œ (ìŠ¤ìº”ë³¸ì€ ì²­í¬ ìƒëµ)
# í…ìŠ¤íŠ¸ pdfëŠ” ì´ê±¸ ìµœì¢…ìœ¼ë¡œ ì´ì œ ìŠ¤ìº”ë³¸ ì²˜ë¦¬í•˜ëŠ”ê±° í•´ì•¼í•¨

import os
import fitz
import re
from dotenv import load_dotenv
from langchain_community.vectorstores import Chroma
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_openai import ChatOpenAI
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain.schema import Document, SystemMessage, HumanMessage

# í™˜ê²½ ì„¤ì •
load_dotenv()
api_key = os.getenv("OPENAI_API_KEY")

pdf_folder = r"C:\Users\user\Desktop\pdfs\20251106"
persist_dir = "./mk_pdf_chroma_db3"

# LLM / ì„ë² ë”© ì„¤ì •
llm = ChatOpenAI(
    model="gpt-4o-mini",
    temperature=0.3,
    max_tokens=8000,
    openai_api_key=api_key
)

embedding_model = HuggingFaceEmbeddings(
    model_name="bespin-global/klue-sroberta-base-continue-learning-by-mnr"
)

# PDF â†’ ë§ˆí¬ë‹¤ìš´ ë³€í™˜
def pdf_to_markdown(pdf_path):
    md_text = ""
    try:
        with fitz.open(pdf_path) as pdf:
            for i, page in enumerate(pdf):
                blocks = page.get_text("blocks")
                blocks = sorted(blocks, key=lambda b: (b[1], b[0]))
                page_text = ""
                for block in blocks:
                    text = block[4].strip()
                    if not text:
                        continue
                    num_words = len(text.split())
                    num_lines = text.count("\n") + 1
                    if num_words <= 5 and num_lines <= 2:
                        page_text += f"# {text}\n\n"
                    elif num_words <= 15:
                        page_text += f"## {text}\n\n"
                    else:
                        page_text += f"{text}\n\n"
                md_text += f"# Page {i + 1}\n\n{page_text}"

        return md_text
    except Exception:
        print(f"[ì˜¤ë¥˜] PDF ë³€í™˜ ì‹¤íŒ¨ : {pdf_path}") 
        return ""

def load_pdf_safe(pdf_path, min_text_len=20):
    """
    PDFë¥¼ ì½ì–´ì„œ Documentë¡œ ë³€í™˜.
    í…ìŠ¤íŠ¸ ê¸¸ì´ê°€ min_text_lenë³´ë‹¤ ì‘ìœ¼ë©´ None ë°˜í™˜ (ìŠ¤ìº”ë³¸ íŒë‹¨)
    """
    md_text = pdf_to_markdown(pdf_path)
    if md_text and len(md_text.strip()) >= min_text_len:
        return [Document(page_content=md_text,
                         metadata={"source": os.path.splitext(os.path.basename(pdf_path))[0]})]
    else:
        return None  # í…ìŠ¤íŠ¸ê°€ ë„ˆë¬´ ì ìœ¼ë©´ ì‹¤íŒ¨ë¡œ ê°„ì£¼

def load_all_pdfs(pdf_folder):
    all_docs = []
    failed_pdfs = []  # í…ìŠ¤íŠ¸ ë¶€ì¡± PDF ëª©ë¡
    pdf_files = [f for f in os.listdir(pdf_folder) if f.lower().endswith(".pdf")]
    print(f"ì´ PDF íŒŒì¼ ìˆ˜: {len(pdf_files)}\n")
    
    for pdf_file in pdf_files:
        pdf_path = os.path.join(pdf_folder, pdf_file)
        try:
            docs = load_pdf_safe(pdf_path)
            if not docs:  # í…ìŠ¤íŠ¸ ë¶€ì¡± â†’ ì²­í¬ ìƒì„± ë¶ˆê°€
                failed_pdfs.append(pdf_file)
                print(f"{pdf_file} â†’ í…ìŠ¤íŠ¸ ë¶€ì¡± / ì²­í¬ ìƒì„± ë¶ˆê°€")
            else:
                all_docs.extend(docs)
                print(f"{pdf_file} ì²˜ë¦¬ ì™„ë£Œ ({len(docs)} ë¬¸ì„œ)")
        except Exception as e:
            print(f"{pdf_file} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
            failed_pdfs.append(pdf_file)
    
    print(f"\nì´ Document ìˆ˜: {len(all_docs)}")
    if failed_pdfs:
        print("\n--- í…ìŠ¤íŠ¸ ë³€í™˜ ì‹¤íŒ¨ / ì²­í¬ ìƒì„± ë¶ˆê°€ PDF ëª©ë¡ ---")
        for f in failed_pdfs:
            print(f"- {f}")
    
    return all_docs

# Vector DB êµ¬ì¶• (DB ì¬ì‚¬ìš©)
if not os.path.exists(persist_dir) or not os.listdir(persist_dir):
    print("DB ìƒˆë¡œ ìƒì„±")
    docs = load_all_pdfs(pdf_folder)
    if docs:
        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1300, chunk_overlap=300)
        split_docs = text_splitter.split_documents(docs)
        print(f"ì´ ì²­í¬ ìˆ˜: {len(split_docs)}")
        vectorstore = Chroma.from_documents(
            documents=split_docs,
            embedding=embedding_model,
            persist_directory=persist_dir
        )
        vectorstore.persist()
    else:
        print("ì²­í¬ ìƒì„± ê°€ëŠ¥í•œ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤. DB ìƒì„± ìƒëµ.")
        vectorstore = None
else:
    print("ê¸°ì¡´ DBë¥¼ ì¬ì‚¬ìš©")
    vectorstore = Chroma(persist_directory=persist_dir, embedding_function=embedding_model)

if vectorstore:
    # MMR ê²€ìƒ‰ ê¸°ë°˜ Retriever
    retriever = vectorstore.as_retriever(
        search_type="similarity",
        search_kwargs={"k": 10}
    )
else:
    retriever = None

# RAG ê¸°ë°˜ ë‹µë³€ ìƒì„±
def rag_answer(question):
    if not retriever:
        return "DBê°€ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € í…ìŠ¤íŠ¸ê°€ ìˆëŠ” PDFë¥¼ ì²˜ë¦¬í•˜ì„¸ìš”."

    retriever_docs = retriever.invoke(question)
    if isinstance(retriever_docs, Document):
        retriever_docs = [retriever_docs]

    context_texts = []
    for doc in retriever_docs:
        content = doc.page_content.strip()
        if not content:
            continue
        context_texts.append(content)
        
    context = "\n\n".join(context_texts)
    
    # ğŸ”¹ ì¶œì²˜ í‘œì‹œ ì‹œ í•„í„°ë§ (í‘œì§€, ì°¨ë¡€, ëª©ì°¨ ë“± ì œê±°)
    skip_keywords = ["í‘œì§€", "ì°¨ë¡€", "ëª©ì°¨", "contents", "ì„œë¬¸", "ë¨¸ë¦¬ë§", "ë°œê°„ì‚¬"]
    sources_used = []
    for doc in retriever_docs:
        content = doc.page_content.strip()
        # í‚¤ì›Œë“œ í¬í•¨ ë˜ëŠ” í…ìŠ¤íŠ¸ ì§§ìœ¼ë©´ ì œì™¸
        if any(k in content[:300].lower() for k in skip_keywords) or len(content) < 100:
            continue
        source_name = doc.metadata.get("source", "ì¶œì²˜ ì—†ìŒ")
        if source_name not in sources_used:
            sources_used.append(source_name)

    # ìµœì†Œ 4ê°œ í™•ë³´ ìœ„í•´ ìœ ì‚¬ë„ ìˆœìœ¼ë¡œ ì¶”ê°€
    if len(sources_used) < 4:
        all_sources = [doc.metadata.get("source", "ì¶œì²˜ ì—†ìŒ") for doc in retriever_docs]
        for s in all_sources:
            if len(sources_used) >= 4:
                break
            if s not in sources_used:
                sources_used.append(s)

    sources_to_show = sources_used[:5]


    messages = [
        SystemMessage(content="""
            ë‹¹ì‹ ì€ ì—¬ëŸ¬ PDF ë¬¸ì„œë¥¼ ì°¸ê³ í•˜ì—¬ ì§ˆë¬¸ì— ë‹µí•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
            - ì œê³µëœ ë¬¸ì„œ ë‚´ìš©ë§Œ í™œìš©í•´ì„œ ë‹µë³€í•˜ì„¸ìš”.
            - ë¬¸ì„œì— ì—†ëŠ” ë‚´ìš©ì€ ì ˆëŒ€ ì¶”ê°€í•˜ì§€ ë§ê³ , ì—†ìœ¼ë©´ 'ì •ë³´ ì—†ìŒ'ì´ë¼ê³  í‘œì‹œí•˜ì„¸ìš”.
            - ë‹µë³€ì€ í•­ëª©ë³„ë¡œ êµ¬ë¶„í•˜ê³ , ê° í•­ëª©ë§ˆë‹¤ ì¶©ë¶„í•œ ì„¤ëª…ê³¼ ì˜ˆì‹œë¥¼ í¬í•¨í•˜ì„¸ìš”.
            - ìµœì†Œ 300ë‹¨ì–´ ì´ìƒ ì‘ì„±í•˜ê³ , ê°€ëŠ¥í•œ í•œ ë¬¸ì„œ ë‚´ìš©ì„ í’ë¶€í•˜ê²Œ í†µí•©í•˜ì„¸ìš”.
            - ì—¬ëŸ¬ ë¬¸ì„œì˜ ë‚´ìš©ì„ ì¢…í•©í•´ í•œ ë¬¸ë‹¨ ì´ìƒì˜ ìì„¸í•œ ë‹µë³€ì„ ì‘ì„±í•˜ì„¸ìš”.
            - ì§ˆë¬¸ì— ë“±ì¥í•˜ëŠ” ë‹¨ì–´ë¥¼ ê°ê° êµ¬ë¶„í•˜ì—¬ ì •í™•íˆ ë‹µë³€í•˜ì„¸ìš”.
            - ê°€ëŠ¥í•œ í•œ ë¬¸ì„œ ë‚´ ë¬¸ë§¥ê³¼ í‚¤ì›Œë“œì— ê¸°ë°˜í•˜ì—¬ ì •í™•í•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”.
            """),
        HumanMessage(content=f"ë¬¸ì„œ ë‚´ìš©:\n{context}\n\nì§ˆë¬¸:\n{question}")
    ]

    response = llm.invoke(messages)
    answer = response.content

    # í›„ì²˜ë¦¬: ì œëª©ì— ë²ˆí˜¸ ë¶™ì´ê³ , ì¶œì²˜ ë§¨ ì•„ë˜
    lines = answer.split("\n")
    final_lines = []

    for line in lines:
        stripped = line.strip()
        if not stripped:
            final_lines.append("")
            continue
        # '###' ì œê±°
        stripped = stripped.lstrip("#").strip()
        # ìˆ«ì + ì  ì œê±° (ì˜ˆ: 1. 2. 3. â€¦)
        stripped = re.sub(r"^\d+\.\s*", "", stripped)
        # **êµµì€ê¸€ì”¨** ì œê±°
        stripped = re.sub(r"\*\*(.*?)\*\*", r"\1", stripped)
        # *ê¸°ìš¸ì„* ì œê±°
        stripped = re.sub(r"\*(.*?)\*", r"\1", stripped)
        final_lines.append(stripped)

    if sources_to_show:
        final_lines.append("")
        final_lines.append("-"*50)
        for s in sources_to_show:
            final_lines.append(f"[ì¶œì²˜: {s}]")

    return "\n".join(final_lines)

# ì‹¤í–‰
if __name__ == "__main__":
    print("=" * 60)
    print("RAG ì§ˆì˜ì‘ë‹µ ì‹œìŠ¤í…œ")
    print("=" * 60)

    while True:
        query = input("\nì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš” (exit ì…ë ¥ ì‹œ ì¢…ë£Œ): ").strip()
        if query.lower() == "exit":
            print("í”„ë¡œê·¸ë¨ ì¢…ë£Œ.")
            break

        answer = rag_answer(query)
        print("\n[ë‹µë³€]:\n" + answer)
        print("-" * 50)
